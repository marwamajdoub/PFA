{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"Concrete_Data - Sheet1.csv\"  # Update the path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features (all columns except target)\n",
    "X = df.drop(columns=['Concrete compressive strength'])\n",
    "\n",
    "# Normalize data for GANs (Min-Max Scaling to [0, 1])\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define dimensions\n",
    "latent_dim = 16  # Noise vector size\n",
    "\n",
    "# Build Generator\n",
    "def build_generator():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\", input_shape=(latent_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(X_train.shape[1], activation=\"sigmoid\")  # Output same as number of features\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build Discriminator\n",
    "def build_discriminator():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")  # Binary classification (real or fake)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create Generator and Discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile Discriminator\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Build and Compile GAN\n",
    "discriminator.trainable = False  # Freeze Discriminator during GAN training\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "fake_data = generator(gan_input)\n",
    "gan_output = discriminator(fake_data)\n",
    "\n",
    "gan = keras.Model(gan_input, gan_output)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train GAN\n",
    "batch_size = 32\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Generate fake samples\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "\n",
    "    # Select real samples\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_data = X_train[idx]\n",
    "\n",
    "    # Labels for real and fake data\n",
    "    real_labels = np.ones((batch_size, 1))\n",
    "    fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Train Discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    valid_labels = np.ones((batch_size, 1))  # Trick discriminator\n",
    "    g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "    # Print training progress\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch} - D Loss: {d_loss[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Generate new synthetic data\n",
    "noise = np.random.normal(0, 1, (10, latent_dim))  # Generate 10 synthetic samples\n",
    "synthetic_data = generator.predict(noise)\n",
    "synthetic_data = scaler.inverse_transform(synthetic_data)  # Convert back to original scale\n",
    "\n",
    "print(\"\\nGenerated Synthetic Concrete Data (First 5 Rows):\")\n",
    "print(pd.DataFrame(synthetic_data, columns=df.columns[:-1]).head())\n",
    "\n",
    "# Plot Real vs Fake Data Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(X_train.flatten(), bins=50, alpha=0.6, label=\"Real Data\")\n",
    "plt.hist(synthetic_data.flatten(), bins=50, alpha=0.6, label=\"Generated Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Real vs Generated Data Distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
